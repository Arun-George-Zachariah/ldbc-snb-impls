/* 
 * Copyright (C) 2016 Stanford University
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
package net.ellitron.ldbcsnbimpls.interactive.torc.util;

import net.ellitron.ldbcsnbimpls.interactive.core.SnbEntity;
import net.ellitron.ldbcsnbimpls.interactive.core.SnbRelation;
import net.ellitron.ldbcsnbimpls.interactive.torc.TorcEntity;
import net.ellitron.torc.TorcGraph;
import net.ellitron.torc.TorcVertex;
import net.ellitron.torc.util.UInt128;

import org.apache.tinkerpop.gremlin.structure.T;
import org.apache.tinkerpop.gremlin.structure.Graph;
import org.apache.tinkerpop.gremlin.structure.VertexProperty;

import org.docopt.Docopt;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.TimeZone;
import java.util.function.Consumer;
import java.util.logging.Level;
import java.util.logging.Logger;

/**
 * A utility for loading dataset files generated by the LDBC SNB Data
 * Generator[1] into TorcDB[2].
 * <p>
 * TODO:<br>
 * <ul>
 * <li>Make file searching more intelligent.</li>
 * </ul>
 * <p>
 * [1]: git@github.com:ldbc/ldbc_snb_datagen.git<br>
 * [2]: git@github.com:ellitron/torc.git<br>
 *
 * @author Jonathan Ellithorpe (jde@cs.stanford.edu)
 */
public class GraphLoader {

  private static final String doc =
      "GraphLoader: A utility for loading dataset files generated by the\n"
      + "LDBC SNB Data Generator into TorcDB. Nodes and edges are loaded\n"
      + "separately using the \"nodes\" or \"edges\" command.\n Nodes must\n"
      + "be loaded first before edges can be loaded.\n"
      + "\n"
      + "Usage:\n"
      + "  GraphLoader [options] nodes SOURCE\n"
      + "  GraphLoader [options] edges SOURCE\n"
      + "  GraphLoader (-h | --help)\n"
      + "  GraphLoader --version\n"
      + "\n"
      + "Arguments:\n"
      + "  SOURCE  Directory containing SNB dataset files.\n"
      + "\n"
      + "Options:\n"
      + "  --coordLoc=<loc>  RAMCloud coordinator locator string\n"
      + "                    [default: tcp:host=127.0.0.1,port=12246].\n"
      + "  --masters=<n>     Number of RAMCloud master servers to use to\n"
      + "                    store the graph [default: 1].\n"
      + "  --graphName=<g>   The name to give the graph in RAMCloud\n"
      + "                    [default: graph].\n"
      + "  --numLoaders=<n>  The total number of loader instances loading\n"
      + "                    the graph in parallel [default: 1].\n"
      + "  --loaderIdx=<n>   Among numLoaders instance, which loader this\n"
      + "                    instance represents. Defines the partition of\n"
      + "                    the dataset this loader instance is responsible\n"
      + "                    for loading. Indexes start from 0.\n"
      + "                    [default: 0].\n"
      + "  --numThreads=<n>  The number of threads to use in this loader\n"
      + "                    instance.\n This loader's dataset partition is\n"
      + "                    divided up among this number of threads."
      + "                    [default: 1].\n"
      + "  --txSize=<n>      How many vertices/edges to load in a single\n"
      + "                    transaction. TorcDB transactions are buffered\n"
      + "                    locally before commit, and written in batch at\n"
      + "                    commit time. Setting this number appropriately\n"
      + "                    can therefore help to ammortize the RAMCloud\n"
      + "                    communication costs and increase loading\n"
      + "                    performance, although the right setting will\n"
      + "                    depend on system setup. [default: 128].\n"
      + "  --txRetries=<r>   The number of times to retry a transaction\n"
      + "                    before giving up. Transactions to RAMCloud may\n"
      + "                    fail due to timeouts, or sometimes conflicts\n"
      + "                    on objects (i.e. in the case of multithreaded\n"
      + "                    loading) [default: 100].\n"
      + "  --splitSfx=<sf>   The presence of this option indicates that the\n"
      + "                    dataset files have been split with each part\n"
      + "                    still containing the original file's header as\n"
      + "                    the first line. The value of this option is a\n"
      + "                    format string for the suffix used for the\n"
      + "                    parts. For example, .part%2d for part suffixes\n"
      + "                    that look like .part00, .part01, etc.\n The\n"
      + "                    presence of this option indicates that ALL\n"
      + "                    files have been split in this way. Also note\n"
      + "                    that this string must contain exactly 1 %d\n"
      + "                    somewhere in the string.\n"
      + "  --reportInt=<i>   Number of seconds between reporting status to\n"
      + "                    the screen. [default: 10].\n"
      + "  -h --help         Show this screen.\n"
      + "  --version         Show version.\n"
      + "\n";

  /*
   * Used for parsing dates in the original dataset files output by the data
   * generator, and converting them to milliseconds since Jan. 1 9170. We store
   * dates in this form in TorcDB.
   */
  private static final SimpleDateFormat birthdayDateFormat;
  private static final SimpleDateFormat creationDateDateFormat;

  static {
    birthdayDateFormat =
        new SimpleDateFormat("yyyy-MM-dd");
    birthdayDateFormat.setTimeZone(TimeZone.getTimeZone("GMT"));
    creationDateDateFormat =
        new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZ");
    creationDateDateFormat.setTimeZone(TimeZone.getTimeZone("GMT"));
  }

  /**
   * Packs a unit of loading work for a loader thread to do. Includes a path to
   * the file to load, and what that file represents (either an SNB entity or
   * an SNB relation).
   */
  private static class LoadUnit {

    private SnbEntity entity;
    private SnbRelation relation;
    private Path filePath;

    public LoadUnit(SnbEntity entity, Path filePath) {
      this.entity = entity;
      this.relation = null;
      this.filePath = filePath;
    }

    public LoadUnit(SnbRelation relation, Path filePath) {
      this.entity = null;
      this.relation = relation;
      this.filePath = filePath;
    }

    public boolean isEntity() {
      return entity != null;
    }

    public boolean isRelation() {
      return relation != null;
    }

    public SnbEntity getSnbEntity() {
      return entity;
    }

    public SnbRelation getSnbRelation() {
      return relation;
    }

    public Path getFilePath() {
      return filePath;
    }
  }

  /**
   * A set of per-thread loading statistics. Each loader thread continually
   * updates these statistics, while a statistics reporting thread with a
   * reference to each ThreadStats instance in the system regularly prints
   * statistic summaries to the screen.
   */
  private static class ThreadStats {

    /*
     * The total number of lines this thread has successfully processed and
     * loaded into the database.
     */
    public long linesProcessed = 0;

    /*
     * The total number of files this thread has successfully processed and
     * loaded into the database.
     */
    public long filesProcessed = 0;

    /*
     * Number of times this thread has attempted to commit a transaction but
     * the transaction failed.
     */
    public long txFailures = 0;
  }

  /**
   * A loader thread which takes a set of files to load and loads them
   * sequentially.
   */
  private static class LoaderThread implements Runnable {

    private Graph graph;
    private List<LoadUnit> loadList;
    private int startIndex;
    private int length;
    private int txSize;
    private int txRetries;
    private ThreadStats stats;

    /**
     * Constructor for LoaderThread.
     *
     * @param graph Graph into which to load the files.
     * @param loadList Master list of all files in the dataset. All loader
     * threads have a copy of this.
     * @param startIndex The index in the load list at which this thread's
     * loading work starts.
     * @param length The segment length in the load list for this thread.
     * @param txSize The number of lines to process within a single
     * transaction.
     * @param txRetries The maximum number of times to retry a failed
     * transaction before giving up and exiting the program.
     * @param stats ThreadStats instance to update with loading statistics
     * info.
     */
    public LoaderThread(Graph graph, List<LoadUnit> loadList, int startIndex,
        int length, int txSize, int txRetries, ThreadStats stats) {
      this.graph = graph;
      this.loadList = loadList;
      this.startIndex = startIndex;
      this.length = length;
      this.txSize = txSize;
      this.txRetries = txRetries;
      this.stats = stats;
    }

    @Override
    public void run() {
      // Load every file in the range [startIndex, startIndex + length)
      for (int fIndex = startIndex; fIndex < startIndex + length; fIndex++) {
        LoadUnit loadUnit = loadList.get(fIndex);
        Path path = loadUnit.getFilePath();

        BufferedReader inFile;
        try {
          inFile = Files.newBufferedReader(path, StandardCharsets.UTF_8);
        } catch (IOException ex) {
          throw new RuntimeException(String.format("Encountered error opening "
              + "file %s", path.getFileName()));
        }

        // First line of the file contains the column headers.
        String[] fieldNames;
        try {
          fieldNames = inFile.readLine().split("\\|");
        } catch (IOException ex) {
          throw new RuntimeException(String.format("Encountered error reading "
              + "header line of file %s", path.getFileName()));
        }

        Consumer<List<String>> lineGobbler;
        if (loadUnit.isEntity()) {
          SnbEntity snbEntity = loadUnit.getSnbEntity();

          long idSpace = TorcEntity.valueOf(snbEntity).idSpace;
          String vertexLabel = TorcEntity.valueOf(snbEntity).label;

          lineGobbler = (List<String> lineBuffer) -> {
            for (int i = 0; i < lineBuffer.size(); i++) {
              /*
               * Here we parse the line into a map of the entity's properties.
               * Date-type fields (birthday, creationDate, ...) need to be
               * converted to the number of milliseconds since January 1, 1970,
               * 00:00:00 GMT. This is the format expected to be returned for
               * these fields by LDBC SNB benchmark queries, although the
               * format in the dataset files are things like "1989-12-04" and
               * "2010-03-17T23:32:10.447+0000". We could do this conversion
               * "live" during the benchmark, but that would detract from the
               * performance numbers' reflection of true database performance
               * since it would add to the client-side query processing
               * overhead.
               */
              String[] fieldValues = lineBuffer.get(i).split("\\|");
              Map<Object, Object> propMap = new HashMap<>();
              for (int j = 0; j < fieldValues.length; j++) {
                try {
                  if (fieldNames[j].equals("id")) {
                    propMap.put(T.id,
                        new UInt128(idSpace, Long.decode(fieldValues[j])));
                  } else if (fieldNames[j].equals("birthday")) {
                    propMap.put(fieldNames[j], String.valueOf(
                        birthdayDateFormat.parse(fieldValues[j])
                        .getTime()));
                  } else if (fieldNames[j].equals("creationDate")) {
                    propMap.put(fieldNames[j], String.valueOf(
                        creationDateDateFormat.parse(fieldValues[j])
                        .getTime()));
                  } else if (fieldNames[j].equals("joinDate")) {
                    propMap.put(fieldNames[j], String.valueOf(
                        creationDateDateFormat.parse(fieldValues[j])
                        .getTime()));
                  } else {
                    propMap.put(fieldNames[j], fieldValues[j]);
                  }
                } catch (Exception ex) {
                  throw new RuntimeException(String.format("Encountered "
                      + "error processing field %s with value %s of line %d "
                      + "in the line buffer. Line: \"%s\"", fieldNames[j], 
                      fieldValues[j], i, lineBuffer.get(i)), ex);
                }
              }

              // Don't forget to add the label!
              propMap.put(T.label, vertexLabel);

              List<Object> keyValues = new ArrayList<>();
              propMap.forEach((key, val) -> {
                keyValues.add(key);
                keyValues.add(val);
              });

              graph.addVertex(keyValues.toArray());
            }
          };
        } else {
          SnbRelation snbRelation = loadUnit.getSnbRelation();

          long tailIdSpace = TorcEntity.valueOf(snbRelation.tail).idSpace;
          long headIdSpace = TorcEntity.valueOf(snbRelation.head).idSpace;
          String edgeLabel = snbRelation.name;

          lineGobbler = (List<String> lineBuffer) -> {
            for (int i = 0; i < lineBuffer.size(); i++) {
              String[] fieldValues = lineBuffer.get(i).split("\\|");

              TorcVertex tailVertex = (TorcVertex) graph.vertices(
                  new UInt128(tailIdSpace, Long.decode(fieldValues[0])))
                  .next();
              TorcVertex headVertex = (TorcVertex) graph.vertices(
                  new UInt128(headIdSpace, Long.decode(fieldValues[1])))
                  .next();

              Map<Object, Object> propMap = new HashMap<>();
              for (int j = 2; j < fieldValues.length; j++) {
                try {
                  if (fieldNames[j].equals("creationDate")
                      || fieldNames[j].equals("joinDate")) {
                    propMap.put(fieldNames[j], String.valueOf(
                        creationDateDateFormat.parse(fieldValues[j])
                        .getTime()));
                  } else {
                    propMap.put(fieldNames[j], fieldValues[j]);
                  }
                } catch (Exception ex) {
                  throw new RuntimeException(String.format("Encountered "
                      + "error processing field %s with value %s of line %d "
                      + "in the line buffer. Line: \"%s\"", fieldNames[j], 
                      fieldValues[j], i, lineBuffer.get(i)), ex);
                }
              }

              List<Object> keyValues = new ArrayList<>();
              propMap.forEach((key, val) -> {
                keyValues.add(key);
                keyValues.add(val);
              });

              tailVertex.addEdge(edgeLabel, headVertex,
                  keyValues.toArray());

              /*
               * If this is not an undirected edge, then add the reverse edge
               * from head to tail.
               */
              if (!snbRelation.directed) {
                headVertex.addEdge(edgeLabel, tailVertex,
                    keyValues.toArray());
              }
            }
          };
        }

        // Keep track of what lines we're on in this file.
        long localLinesProcessed = 0;

        boolean hasLinesLeft = true;
        while (hasLinesLeft) {
          /*
           * Buffer txSize lines at a time from the input file and keep it
           * around until commit time. If the commit succeeds we can forget
           * about it, otherwise we'll use it again to retry the transaction.
           */
          String line;
          List<String> lineBuffer = new ArrayList<>(txSize);
          try {
            while ((line = inFile.readLine()) != null) {
              lineBuffer.add(line);
              if (lineBuffer.size() == txSize) {
                break;
              }
            }
          } catch (IOException ex) {
            throw new RuntimeException(String.format("Encountered error "
              + "reading lines of file %s", path.getFileName()));
          }

          // Catch when we've read all the lines in the file.
          if (line == null) {
            hasLinesLeft = false;
          }

          /*
           * Parse the lines in the buffer and write them into the database. If
           * the commit fails for any reason, retry the transaction up to
           * txRetries number of times.
           */
          int txFailCount = 0;
          while (true) {
            try {
              lineGobbler.accept(lineBuffer);
            } catch (Exception ex) {
              throw new RuntimeException(String.format(
                  "Encountered error processing lines in range [%d, %d] of "
                  + "file %s",
                  localLinesProcessed + 2,
                  localLinesProcessed + 1 + lineBuffer.size(),
                  path.getFileName()), ex);
            }

            try {
              graph.tx().commit();
              localLinesProcessed += lineBuffer.size();
              stats.linesProcessed += lineBuffer.size();
              break;
            } catch (Exception e) {
              /*
               * The transaction failed due to either a conflict or a timeout.
               * In this case we want to retry the transaction, but only up to
               * the txRetries limit.
               */
              txFailCount++;
              stats.txFailures++;

              if (txFailCount > txRetries) {
                throw new RuntimeException(String.format(
                    "Transaction failed %d times on line range [%d, %d] "
                    + "of file %s",
                    txFailCount, localLinesProcessed + 2,
                    localLinesProcessed + 1 + lineBuffer.size(),
                    path.getFileName()));
              }
            }
          }
        }

        try {
          inFile.close();
        } catch (IOException ex) {
          throw new RuntimeException(String.format("Encountered error closing "
              + "file %s", path.getFileName()));
        }
        stats.filesProcessed++;
      }
    }
  }

  /**
   * A thread which reports statistics on the loader threads in the system at a
   * set interval. This thread gets information on each thread via a shared
   * ThreadStats object with each thread.
   */
  private static class StatsReporterThread implements Runnable {

    private List<ThreadStats> threadStats;
    private long reportInterval;

    /**
     * Constructor for StatsReporterThread.
     *
     * @param threadStats List of all the shared ThreadStats objects, one per
     * thread.
     * @param reportInterval Interval, in seconds, to report statistics to the
     * screen.
     */
    public StatsReporterThread(List<ThreadStats> threadStats,
        long reportInterval) {
      this.threadStats = threadStats;
      this.reportInterval = reportInterval;
    }

    @Override
    public void run() {
      try {
        // Print the column headers.
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < threadStats.size(); i++) {
          sb.append(String.format("%12s", "thread" + i));
        }
        sb.append(String.format("%12s", "total"));
        System.out.println(sb.toString());

        long startTime = System.currentTimeMillis();
        while (true) {
          Thread.sleep(reportInterval * 1000l);

          // Time elapsed since beginning loading.
          long timeElapsed = (System.currentTimeMillis() - startTime) / 1000l;

          sb = new StringBuilder();
          long totalAvgLineRate = 0;
          for (int i = 0; i < threadStats.size(); i++) {
            ThreadStats stats = threadStats.get(i);
            long avgLineRate = stats.linesProcessed / timeElapsed;
            sb.append(String.format("%12d", avgLineRate));
            totalAvgLineRate += avgLineRate;
          }
          sb.append(String.format("%12d", totalAvgLineRate));

          System.out.println(sb.toString());
        }
      } catch (InterruptedException ex) {
        // This is fine, we're probably being terminated, in which case we 
        // should just go ahead and terminate.
      }
    }
  }

  public static void main(String[] args)
      throws FileNotFoundException, IOException, ParseException, InterruptedException {
    Map<String, Object> opts =
        new Docopt(doc).withVersion("GraphLoader 1.0").parse(args);

    int numLoaders = Integer.decode((String) opts.get("--numLoaders"));
    int loaderIdx = Integer.decode((String) opts.get("--loaderIdx"));
    int numThreads = Integer.decode((String) opts.get("--numThreads"));
    int txSize = Integer.decode((String) opts.get("--txSize"));
    int txRetries = Integer.decode((String) opts.get("--txRetries"));
    long reportInterval = Long.decode((String) opts.get("--reportInt"));
    String splitSuffix = (String) opts.get("--splitSfx");
    String inputDir = (String) opts.get("SOURCE");

    String command;
    if ((Boolean) opts.get("nodes")) {
      command = "nodes";
    } else {
      command = "edges";
    }

    System.out.println(String.format(
        "GraphLoader: {coordLoc: %s, masters: %s, graphName: %s, "
        + "numLoaders: %d, loaderIdx: %d, numThreads: %d, txSize: %d, "
        + "txRetries: %d, splitSfx: %s, inputDir: %s, command: %s}",
        (String) opts.get("--coordLoc"),
        (String) opts.get("--masters"),
        (String) opts.get("--graphName"),
        numLoaders,
        loaderIdx,
        numThreads,
        txSize,
        txRetries,
        splitSuffix,
        inputDir,
        command));

    // Open a new TorcGraph with the supplied configuration.
    Map<String, String> config = new HashMap<>();
    config.put(TorcGraph.CONFIG_COORD_LOCATOR,
        (String) opts.get("--coordLoc"));
    config.put(TorcGraph.CONFIG_GRAPH_NAME,
        (String) opts.get("--graphName"));
    config.put(TorcGraph.CONFIG_NUM_MASTER_SERVERS,
        (String) opts.get("--masters"));

    Graph graph = TorcGraph.open(config);

    /*
     * Construct a list of all the files that need to be loaded. If we are
     * loading nodes then this list will be a list of all the node files that
     * need to be loaded. Otherwise it will be a list of edge files.
     */
    List<LoadUnit> loadList = new ArrayList<>();
    if (command.equals("nodes")) {
      for (SnbEntity snbEntity : SnbEntity.values()) {

        String baseFileName = snbEntity.name + "_0_0.csv";

        if (splitSuffix != null) {
          for (int i = 0;; i++) {
            String fileName =
                String.format("%s" + splitSuffix, baseFileName, i);
            File f = new File(inputDir + "/" + fileName);
            if (f.exists() && f.isFile()) {
              loadList.add(new LoadUnit(snbEntity,
                  Paths.get(inputDir + "/" + fileName)));
            } else {
              if (i == 0) {
                System.out.println(String.format(
                    "Missing file for %s nodes (%s)",
                    snbEntity.name, fileName));
              } else {
                System.out.println(String.format(
                    "Found %d file parts for %s nodes",
                    i, snbEntity.name));
              }
              break;
            }
          }
        } else {
          File f = new File(inputDir + "/" + baseFileName);
          if (f.exists() && f.isFile()) {
            loadList.add(new LoadUnit(snbEntity,
                Paths.get(inputDir + "/" + baseFileName)));
            System.out.println(String.format("Found file for %s nodes (%s)",
                snbEntity.name, baseFileName));
          } else {
            System.out.println(String.format("Missing file for %s nodes (%s)",
                snbEntity.name, baseFileName));
          }
        }
      }

      System.out.println(String.format("Found %d total node files",
          loadList.size()));
    } else {
      for (SnbRelation snbRelation : SnbRelation.values()) {

        String baseFileName = String.format("%s_%s_%s_0_0.csv",
            snbRelation.tail.name,
            snbRelation.name,
            snbRelation.head.name);

        String edgeFormatStr;
        if (snbRelation.directed) {
          edgeFormatStr = "(%s)-[%s]->(%s)";
        } else {
          edgeFormatStr = "(%s)-[%s]-(%s)";
        }

        String edgeStr = String.format(edgeFormatStr,
            snbRelation.tail.name,
            snbRelation.name,
            snbRelation.head.name);

        if (splitSuffix != null) {
          for (int i = 0;; i++) {
            String fileName =
                String.format("%s" + splitSuffix, baseFileName, i);
            File f = new File(inputDir + "/" + fileName);
            if (f.exists() && f.isFile()) {
              loadList.add(new LoadUnit(snbRelation,
                  Paths.get(inputDir + "/" + fileName)));
            } else {
              if (i == 0) {
                System.out.println(String.format(
                    "Missing file for %s edges (%s)",
                    edgeStr, fileName));
              } else {
                System.out.println(String.format(
                    "Found %d file parts for %s edges",
                    i, edgeStr));
              }
              break;
            }
          }
        } else {
          File f = new File(inputDir + "/" + baseFileName);
          if (f.exists() && f.isFile()) {
            loadList.add(new LoadUnit(snbRelation,
                Paths.get(inputDir + "/" + baseFileName)));
            System.out.println(String.format("Found file for %s edges (%s)",
                edgeStr, baseFileName));
          } else {
            System.out.println(String.format("Missing file for %s edges (%s)",
                edgeStr, baseFileName));
          }
        }
      }

      System.out.println(String.format("Found %d total edge files",
          loadList.size()));
    }

    /*
     * Calculate the segment of the list that this loader instance is
     * responsible for loading.
     */
    int q = loadList.size() / numLoaders;
    int r = loadList.size() % numLoaders;

    int loadSize;
    int loadOffset;
    if (loaderIdx < r) {
      loadSize = q + 1;
      loadOffset = (q + 1) * loaderIdx;
    } else {
      loadSize = q;
      loadOffset = ((q + 1) * r) + (q * (loaderIdx - r));
    }

    /*
     * Divvy up the load and start the threads.
     */
    List<Thread> threads = new ArrayList<>();
    List<ThreadStats> threadStats = new ArrayList<>(numThreads);
    for (int i = 0; i < numThreads; i++) {
      int qt = loadSize / numThreads;
      int rt = loadSize % numThreads;

      int threadLoadSize;
      int threadLoadOffset;
      if (i < rt) {
        threadLoadSize = qt + 1;
        threadLoadOffset = (qt + 1) * i;
      } else {
        threadLoadSize = qt;
        threadLoadOffset = ((qt + 1) * rt) + (qt * (i - rt));
      }

      threadLoadOffset += loadOffset;

      ThreadStats stats = new ThreadStats();

      threads.add(new Thread(new LoaderThread(graph, loadList,
          threadLoadOffset, threadLoadSize, txSize, txRetries, stats)));

      threads.get(i).start();

      threadStats.add(stats);
    }

    /*
     * Start stats reporting thread.
     */
    (new Thread(new StatsReporterThread(threadStats, reportInterval))).start();

    /*
     * Join on all the loader threads.
     */
    for (Thread thread : threads) {
      thread.join();
    }
  }
}
